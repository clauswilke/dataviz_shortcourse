---
title: "Visualizing Uncertainty and Trends"
author: "Claus O. Wilke"
date: "last updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, "Wilke-slides-theme.css"]
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      slideNumberFormat: ''
      titleSlideClass: [center, middle]
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment = "")

library(tidyverse)
library(here)
library(colorspace)
library(cowplot)

hpi_trends <- read_csv(here("datasets", "hpi_trends.csv"))
co2 <- read_csv(here("datasets", "co2.csv")) |>
  filter(year >= 1959, year < 2024) # use complete years only
```


## We all know how to visualize uncertainty, right?

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/butterfat-bars-1.png")
background-position: left 50% top 65%
background-size: 50%

## We all know how to visualize uncertainty, right?

.absolute-bottom-right.tiny-font[
Milk butterfat contents by cattle breed. 
Source: Canadian Record of Performance for Purebred Dairy Cattle
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/median-age-income-1.png")
background-position: left 50% top 65%
background-size: 50%

## We all know how to visualize uncertainty, right?

.absolute-bottom-right.tiny-font[
Income versus age for 67 counties in Pennsylvania. 
Source: 2015 Five-Year American Community Survey
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/blue-jays-static.png")
background-position: left 50% top 75%
background-size: 50%

## We all know how to visualize uncertainty, right?

.absolute-bottom-right.tiny-font[
Head length versus body mass for male blue jays. Data source: Keith Tarvin, Oberlin College
]

???

Figure redrawn after [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

## Two problems

--

1. It's often not clear what the visualizations represent

--

2. Even if we know, we can have difficulty interpreting them

---


## 1. It's often not clear what the visualizations represent

In particular, error bars can represent many different quantities

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/cocoa-data-vs-CI-1.png")
background-position: left 50% top 65%
background-size: 80%

## Error bars can represent many different quantities

.absolute-bottom-right.tiny-font[
Chocolate bar ratings. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

## 2. We can have difficulty interpreting the visualizations


.absolute-bottom-right.tiny-font[Lace Padilla, Matthew Kay, and Jessica Hullman (2022): [Uncertainty Visualization](
https://osf.io/preprints/psyarxiv/ebd6r)
]


--

- People are not good at reasoning about probabilities

--

- People will substitute uncertainty with simpler concepts  
(Deterministic Construal Error)


---

## 2. We can have difficulty interpreting the visualizations

- People are not good at reasoning about probabilities

- People will substitute uncertainty with simpler concepts  
(Deterministic Construal Error)

- People will interpret ranges as boundaries

.absolute-bottom-right.tiny-font[Lace Padilla, Matthew Kay, and Jessica Hullman (2022): [Uncertainty Visualization](
https://osf.io/preprints/psyarxiv/ebd6r)
]


---

background-image: url("uncertainty_files/confidence-visualizations-3.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars with caps

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/confidence-visualizations-3.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars with caps

Determinstic Construal Error: Error bars are interpreted as min/max values

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

background-image: url("uncertainty_files/confidence-visualizations-3.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars with caps

Categorical thinking: Areas outside and inside the error bars are categorically different

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

background-image: url("uncertainty_files/confidence-visualizations-4.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars without caps

You can remove caps to make the boundary visually less severe

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/confidence-visualizations-2.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as graded error bars

You can show multiple confidence levels to de-emphasize existence of boundary

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

background-image: url("uncertainty_files/confidence-visualizations-5.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as confidence strips

You can use faded strips (but hard to read/interpret)

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/confidence-visualizations-6.png")
background-position: left 50% top 75%
background-size: 60%

## Uncertainty shown as distributions

You can show actual distributions  
Popular in Bayesian inference, but still difficult to interpret

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

## Hypothetical Outcome Plots

What would you experience comparing two random chocolate bars?

.absolute-bottom-right.tiny-font[
[Hullman et al., PLOS ONE 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444)
]

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/chocolate-HOP-animated-1.gif")
background-position: left 50% top 70%
background-size: 60%

## Hypothetical Outcome Plots

What would you experience comparing two random chocolate bars?

.absolute-bottom-right.tiny-font[
[Hullman et al., PLOS ONE 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444)
]

---

## Use frequency framing to reason about probabilities

.center[
```{r freq-waffle, out.width = "40%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
g <- expand.grid(x = 1:10, y = 1:10)

set.seed(84520)
data <- data.frame(ratio = c(0.99, 0.9, 0.7, 0.3, 0.1)) %>%
  mutate(
    out = purrr::map(
      ratio,
      ~g %>% mutate(
        value = {
          n <- n()
          i <- round(n*.x)
          sample(c(rep("W", i), rep("L", n - i)), n)
        }
      )
    )
  ) %>%
  unnest(cols = out) %>%
  mutate(
    label = paste0(round(100*(1-ratio)), "% chance")
  )

data %>% filter(ratio == .99) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to reason about probabilities

.center[
```{r freq-waffle2, out.width = "40%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .9) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to reason about probabilities

.center[
```{r freq-waffle3, out.width = "40%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .7) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to reason about probabilities

.center[
```{r freq-waffle4, out.width = "40%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .3) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to reason about probabilities

.center[
```{r freq-waffle5, out.width = "40%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .1) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

background-image: url("uncertainty_files/blue-jays-static.png")
background-position: left 50% top 75%
background-size: 50%


## Let's consider uncertainty of trend lines

.move-up-1em[
What does the confidence band show?
]

.absolute-bottom-right.tiny-font[
Head length versus body mass for male blue jays. Data source: Keith Tarvin, Oberlin College
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/blue-jays-HOP.gif")
background-position: left 50% top 75%
background-size: 50%

## Let's consider uncertainty of trend lines

.move-up-1em[
Both the intercept and the slope have uncertainty
]

.absolute-bottom-right.tiny-font[
Head length versus body mass for male blue jays. Data source: Keith Tarvin, Oberlin College
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/mpg-uncertain-1.png")
background-position: left 50% top 65%
background-size: 80%

## It gets even more confusing for non-linear trend lines

.move-up-1em[
Individual posterior samples tend to be more wiggly than the mean
]

.absolute-bottom-right.tiny-font[
Fuel efficiency versus displacement, for 32 cars (1973–74 models). Source: Motor Trend, 1974.
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/mpg-uncertain-HOP-static-1.png")
background-position: left 50% top 77%
background-size: 75%

## It gets even more confusing for non-linear trend lines

.move-up-1em[
Individual posterior samples tend to be more wiggly than the mean
]

.absolute-bottom-right.tiny-font[
Fuel efficiency versus displacement, for 32 cars (1973–74 models). Source: Motor Trend, 1974.
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---


background-image: url("dataviz/visualizing_uncertainty_files/figure-html/mpg-uncertain-HOP-animated-1.gif")
background-position: left 50% top 70%
background-size: 50%

## Hypothetical Outcome Plots help develop intuition

.absolute-bottom-right.tiny-font[
Fuel efficiency versus displacement, for 32 cars (1973–74 models). Source: Motor Trend, 1974.
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)




---

## Detrending: Removing the underlying trend

.center[
```{r hpi-no-trendline, echo = FALSE, warning = FALSE, out.width = "55%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date, hpi)) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_date(name = NULL) +
  scale_y_log10(name = "House Price Index (Dec. 2000 = 100)") +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

--

.small-font[
Did housing prices in California decline substantially from 1990 to 1998?
]

--

.small-font[
Did housing prices in West Virginia recover by 2017?
]

???

Figure redrawn after [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

## Detrending: Removing the underlying trend

.center[
```{r hpi-trends, echo = FALSE, warning = FALSE, out.width = "55%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date, hpi)) +
  geom_line(aes(y = hpi_trend), color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_date(name = NULL) +
  scale_y_log10(name = "House Price Index (Dec. 2000 = 100)") +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.small-font[
Did housing prices in California decline substantially from 1990 to 1998?
]

.small-font[
Did housing prices in West Virginia recover by 2017?
]

???

Figure redrawn after [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

## Detrending: Removing the underlying trend

.center[
```{r hpi-detrended, echo = FALSE, warning = FALSE, out.width = "55%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date, hpi_detrended)) +
  geom_hline(yintercept = 1, color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_date(name = NULL) +
  scale_y_log10(
    name = "House Price Index (detrended)",
    breaks = c(0.752, 1, 1.33, 1.77),
    labels = c("0.75", "1.00", "1.33", "1.77")
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.small-font[
Did housing prices in California decline substantially from 1990 to 1998? — yes
]

.small-font[
Did housing prices in West Virginia recover by 2017?  — no
]

???

Figure redrawn after [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

## Title

.center[
```{r keeling-curve, echo = FALSE, warning = FALSE, out.width = "65%", fig.width = 6, fig.asp = 0.618, dev = "svg"}
# convert to time series object
co2_ts <- ts(
  data = co2$co2_ave,
  start = 1959,       # data starts Jan 1959
  end = c(2023, 12),  # data ends Dec 2023
  frequency = 12      # we have 12 time points per year
)

# detrend via STL method
# Seasonal Decomposition of Time Series by Loess
# s.window is the span of the loess window; should be odd and at least 7
co2_stl <- stl(co2_ts, s.window = 7)

co2_detrended <- mutate(
  co2,
  seasonal = t(co2_stl$time.series)[1, ],
  trend = t(co2_stl$time.series)[2, ],
  remainder = t(co2_stl$time.series)[3, ]
)

ggplot(co2_detrended, aes(date_dec, co2_ave)) +
  geom_line(aes(color = "monthly"), linewidth = 0.6) +
  geom_line(aes(y = trend, color = "trend"), linewidth = 0.6) +
  scale_color_manual(
    name = NULL,
    values = c(monthly = "#0072B2", trend = "#D55E00"),
    breaks = c("monthly", "trend"),
    labels = c("monthly average  ", "long-term trend"),
    guide = guide_legend(
      override.aes = list(
        linewidth = 1
      )
    )
  ) +
  scale_y_continuous(
    limits = c(295, 430),
    breaks = c(300, 325, 350, 375, 400, 425),
    labels = c("300", "", "350", "", "400", ""),
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(1958, 2024),
    name = NULL,
    breaks = c(1960, 1970, 1980, 1990, 2000, 2010, 2020),
    labels = c("1960", "", "1980", "", "2000", "", "2020"),
    expand = c(0, 0)
  ) +
  theme_minimal_grid(14) +
  theme(
    legend.position = "top",
    legend.justification = "right",
    legend.box.spacing = unit(3.5, "pt"), # distance between legend and plot
    legend.background = element_rect(fill = "white", color = NA),
    legend.key.width = unit(14, "pt")
  )
```
]


---

## Title

.center[
```{r keeling-curve-zoomed, echo = FALSE, warning = FALSE, out.width = "65%", fig.width = 6, fig.asp = 0.618, dev = "svg"}
ggplot(co2_detrended, aes(date_dec, co2_ave)) +
  geom_line(aes(color = "monthly"), linewidth = 1) +
  geom_line(aes(y = trend, color = "trend"), linewidth = 1) +
  scale_color_manual(
    name = NULL,
    values = c(monthly = "#0072B2", trend = "#D55E00"),
    breaks = c("monthly", "trend"),
    labels = c("monthly average  ", "long-term trend")
  ) +
  scale_y_continuous(
    limits = c(383, 404),
    breaks = c(385, 390, 395, 400),
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(2009.5, 2014.5),
    name = NULL,
    breaks = 2010:2014,
    expand = c(0, 0)
  ) +
  theme_minimal_grid(14) +
  theme(
    legend.position = "top",
    legend.justification = "right",
    legend.box.spacing = unit(3.5, "pt"), # distance between legend and plot
    legend.background = element_rect(fill = "white", color = NA),
    legend.key.width = unit(14, "pt")
  )
```
]

---

.center[
```{r keeling-curve-decomposition, echo = FALSE, warning = FALSE, out.width = "85%", fig.width = 9, fig.asp = 0.618, dev = "svg"}
facet_labels <- c("monthly average", "long-term trend", "seasonal fluctuations", "remainder")

co2_detrended |>
  rename(
    "monthly average" = co2_ave,
    "long-term trend" = trend,
    "seasonal fluctuations" = seasonal
  ) |>
  select(date_dec, `monthly average`, `seasonal fluctuations`, `long-term trend`, remainder) |>
  pivot_longer(-date_dec, names_to = "variable", values_to = "value") |>
  mutate(variable = factor(variable, levels = facet_labels)) |>
  filter(date_dec >= 1989) |>
  ggplot(aes(date_dec, value)) +
  geom_line(color = "#0072B2", linewidth = 0.6) +
  geom_point( # ghost points for scale range
    data = data.frame(
      variable = factor(
        rep(facet_labels, each = 2),
        levels = facet_labels
      ),
      x = 1990,
      y = c(324, 419, 324, 419, -4.1, 4.3, -.81, .85)
    ),
    aes(x, y),
    color = NA,
    na.rm = TRUE
  ) +
  scale_y_continuous(
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(1989, 2023.2),
    name = NULL,
    breaks = c(1990, 1995, 2000, 2005, 2010, 2015, 2020),
    labels = c("1990", "", "2000", "", "2010", "", "2020"),
    expand = c(0, 0)
  ) +
  facet_wrap(facets = vars(variable), ncol = 1, scales = "free") +
  theme_minimal_grid() +
  theme(
    plot.margin = margin(3, 1.5, 3, 1.5),
    strip.text = element_text(size = 12)
  )
```
]

---

.center[
```{r keeling-curve-decomposition-zoomed, echo = FALSE, warning = FALSE, out.width = "85%", fig.width = 9, fig.asp = 0.618, dev = "svg"}
facet_labels <- c("monthly average", "long-term trend", "seasonal fluctuations", "remainder")

co2_detrended |>
  rename(
    "monthly average" = co2_ave,
    "long-term trend" = trend,
    "seasonal fluctuations" = seasonal
  ) |>
  select(date_dec, `monthly average`, `seasonal fluctuations`, `long-term trend`, remainder) |>
  pivot_longer(-date_dec, names_to = "variable", values_to = "value") |>
  mutate(variable = factor(variable, levels = facet_labels)) |>
  filter(date_dec > 2009.5 & date_dec < 2014.5) |>
  ggplot(aes(date_dec, value)) +
  geom_line(color = "#0072B2", linewidth = 0.6) +
  geom_point( # ghost points for scale range
    data = data.frame(
      variable = factor(
        rep(facet_labels, each = 2),
        levels = facet_labels
      ),
      x = 2012,
      y = c(383, 404, 383, 404, -5, 5, -1, 1)
    ),
    aes(x, y),
    color = NA,
    na.rm = TRUE
  ) +
  scale_y_continuous(
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(2009.5, 2014.5),
    name = NULL,
    breaks = 2010:2014,
    expand = c(0, 0)
  ) +
  facet_wrap(facets = vars(variable), ncol = 1, scales = "free") +
  theme_minimal_grid() +
  theme(
    plot.margin = margin(3, 1.5, 3, 1.5),
    strip.text = element_text(size = 12)
  )
```
]


---

## Further reading

- Fundamentals of Data Visualization: [Chapter 16: Visualizing uncertainty](https://clauswilke.com/dataviz/visualizing-uncertainty.html)
- L. Padilla, M. Kay, and J. Hullman (2022). [Uncertainty Visualization](
https://osf.io/preprints/psyarxiv/ebd6r)
- J. Hullman, P. Resnick, and E. Adar (2015). [Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444)


