---
title: "Visualizing Uncertainty and Trends"
author: "Claus O. Wilke"
date: "The University of Texas at Austin"
output:
  xaringan::moon_reader:
    css: [default, "Wilke-slides-theme.css"]
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      slideNumberFormat: ''
      titleSlideClass: [center, middle]
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment = "")

library(tidyverse)
library(here)
library(colorspace)
library(cowplot)

hpi_trends <- read_csv(here("datasets", "fmhpi.csv")) |>
  filter(year >= 1980) |> # trends are weird before 1980
  nest(data = -state) |>
  mutate(
    # linear trend
    fit_lin = map(data, ~lm(hpi ~ date_dec, data = .x)),
    hpi_trend_lin = map2(fit_lin, data, ~predict(.x, .y)),
    # log trend
    fit_log = map(data, ~lm(log(hpi) ~ date_dec, data = .x)),
    hpi_trend_log = map2(fit_log, data, ~exp(predict(.x, .y)))
  ) |>
  select(-fit_lin, -fit_log) |>
  unnest(cols = c(data, hpi_trend_lin, hpi_trend_log))


co2 <- read_csv(here("datasets", "co2.csv")) |>
  filter(year >= 1959, year < 2024) # use complete years only
```


## We all know how to visualize uncertainty, right?

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/butterfat-bars-1.png")
background-position: left 50% top 65%
background-size: 60%

## We all know how to visualize uncertainty, right?

.absolute-bottom-right.tiny-font[
Milk butterfat contents by cattle breed. 
Source: Canadian Record of Performance for Purebred Dairy Cattle
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/median-age-income-1.png")
background-position: left 50% top 65%
background-size: 50%

## We all know how to visualize uncertainty, right?

.absolute-bottom-right.tiny-font[
Income versus age for 67 counties in Pennsylvania. 
Source: 2015 Five-Year American Community Survey
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/blue-jays-static.png")
background-position: left 50% top 75%
background-size: 50%

## We all know how to visualize uncertainty, right?

.absolute-bottom-right.tiny-font[
Head length versus body mass for male blue jays. Data source: Keith Tarvin, Oberlin College
]

???

Figure redrawn after [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

## These commonly used visualizations have problems

--

- It's often not clear what exactly the visualizations represent

--

- Even if we know, we can have difficulty interpreting what we see

---


## It's often not clear what the visualizations represent

--

In particular, error bars can represent many different quantities

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/cocoa-data-vs-CI-1.png")
background-position: left 50% top 65%
background-size: 80%

## It's often not clear what the visualizations represent

.absolute-bottom-right.tiny-font[
Chocolate bar ratings. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

## We can have difficulty interpreting what we see


.absolute-bottom-right.tiny-font[Lace Padilla, Matthew Kay, and Jessica Hullman (2022): [Uncertainty Visualization](
https://osf.io/preprints/psyarxiv/ebd6r)
]


--

- People are not good at reasoning about probabilities

--

- People will substitute uncertainty with simpler concepts  
(Deterministic Construal Error)


---

## We can have difficulty interpreting what we see

- People are not good at reasoning about probabilities

- People will substitute uncertainty with simpler concepts  
(Deterministic Construal Error)

- People will interpret intervals as hard boundaries

.absolute-bottom-right.tiny-font[Lace Padilla, Matthew Kay, and Jessica Hullman (2022): [Uncertainty Visualization](
https://osf.io/preprints/psyarxiv/ebd6r)
]


---

background-image: url("uncertainty_files/confidence-visualizations-3.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars with caps

Chocolate bars from four countries compared to bars from the US

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/confidence-visualizations-3.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars with caps

Determinstic Construal Error: Error bars are interpreted as min/max values

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

background-image: url("uncertainty_files/confidence-visualizations-3.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars with caps

Categorical thinking: Areas outside and inside the error bars are categorically different

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

background-image: url("uncertainty_files/confidence-visualizations-4.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as 95% CI error bars without caps

You can remove caps to make the boundary visually less severe

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/confidence-visualizations-2.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as graded error bars

You can show multiple confidence levels to de-emphasize existence of boundary

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)


---

background-image: url("uncertainty_files/confidence-visualizations-5.png")
background-position: left 50% top 65%
background-size: 60%

## Uncertainty shown as confidence strips

You can use faded strips (but hard to read/interpret)

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/confidence-visualizations-6.png")
background-position: left 50% top 75%
background-size: 60%

## Uncertainty shown as distributions

You can show actual distributions  
Popular in Bayesian inference, but still difficult to interpret

.absolute-bottom-right.tiny-font[
Relative rankings compared to US chocolate bars. Source: Manhattan Chocolate Society
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

## Consider the actual experience of a customer

--

If I can buy either a Canadian or a US bar, what is the probability that the Canadian bar will be better?

--

Answer&colon; The Canadian bar has a 53% chance of being better

--

How can we communicate this?

---

## Use frequency framing to communicate probabilities

.center.move-up-1em[
```{r freq-waffle, out.width = "50%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
g <- expand.grid(x = 1:10, y = 1:10)

set.seed(84520)
data <- data.frame(ratio = c(0.99, 0.9, 0.7, 0.47, 0.3, 0.1)) %>%
  mutate(
    out = purrr::map(
      ratio,
      ~g %>% mutate(
        value = {
          n <- n()
          i <- round(n*.x)
          sample(c(rep("W", i), rep("L", n - i)), n)
        }
      )
    )
  ) %>%
  unnest(cols = out) %>%
  mutate(
    label = paste0(round(100*(1-ratio)), "% chance")
  )

data %>% filter(ratio == .47) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to communicate probabilities

.center.move-up-1em[
```{r freq-waffle2, out.width = "50%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .1) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to communicate probabilities

.center.move-up-1em[
```{r freq-waffle3, out.width = "50%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .3) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to communicate probabilities

.center.move-up-1em[
```{r freq-waffle4, out.width = "50%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .7) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Use frequency framing to communicate probabilities

.center.move-up-1em[
```{r freq-waffle5, out.width = "50%", fig.width = 4.5, fig.asp = 1, echo = FALSE, dev = "svg"}
data %>% filter(ratio == .9) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(color = "white", linewidth = 2) +
  coord_fixed(expand = FALSE, clip = "off") +
  scale_x_continuous(name = NULL, breaks = NULL) +
  scale_y_continuous(name = NULL, breaks = NULL) +
  scale_fill_manual(
    name = NULL,
    breaks = c("W", "L"),
    labels = c("loss   ", "win"),
    values = c(
      "L" = desaturate(darken("#0072B2", .4), .5),
      "W" = desaturate(lighten("#0072B2", .7), .5)
    ),
    guide = guide_legend(override.aes = list(size = 0))
  ) +
  facet_wrap(~label) +
  theme_minimal_grid(20) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.justification = "right",
    legend.box.spacing = unit(12, "pt"),
    legend.spacing.x = unit(3, "pt"),
    legend.key.size = unit(18, "pt")
  )
```
]

---

## Alternatively: Use Hypothetical Outcome Plots

Hypothetical Outcome Plots use animation to let viewers experience uncertainty

.absolute-bottom-right.tiny-font[
[Hullman et al., PLOS ONE 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444)
]

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/chocolate-HOP-animated-1.gif")
background-position: left 50% top 70%
background-size: 60%

## Alternatively: Use Hypothetical Outcome Plots

Hypothetical Outcome Plots use animation to let viewers experience uncertainty

.absolute-bottom-right.tiny-font[
[Hullman et al., PLOS ONE 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444)
]

---
class: middle center

## Moving on to trend lines


---

background-image: url("uncertainty_files/blue-jays-static.png")
background-position: left 50% top 75%
background-size: 50%


## Let's consider the uncertainty of trend lines

.move-up-1em[
What does the confidence band show?
]

.absolute-bottom-right.tiny-font[
Head length versus body mass for male blue jays. Data source: Keith Tarvin, Oberlin College
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("uncertainty_files/blue-jays-HOP.gif")
background-position: left 50% top 75%
background-size: 50%

## Let's consider the uncertainty of trend lines

.move-up-1em[
Both the intercept and the slope have uncertainty
]

.absolute-bottom-right.tiny-font[
Head length versus body mass for male blue jays. Data source: Keith Tarvin, Oberlin College
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/mpg-uncertain-1.png")
background-position: left 50% top 65%
background-size: 80%

## It gets even more confusing for non-linear trend lines

.move-up-1em[
Individual sample fits tend to be more wiggly than the mean
]

.absolute-bottom-right.tiny-font[
Fuel efficiency versus displacement, for 32 cars (1973–74 models). Source: Motor Trend, 1974
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---

background-image: url("dataviz/visualizing_uncertainty_files/figure-html/mpg-uncertain-HOP-static-1.png")
background-position: left 50% top 77%
background-size: 75%

## It gets even more confusing for non-linear trend lines

.move-up-1em[
Individual sample fits tend to be more wiggly than the mean
]

.absolute-bottom-right.tiny-font[
Fuel efficiency versus displacement, for 32 cars (1973–74 models). Source: Motor Trend, 1974
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---


background-image: url("dataviz/visualizing_uncertainty_files/figure-html/mpg-uncertain-HOP-animated-1.gif")
background-position: left 50% top 70%
background-size: 50%

## Hypothetical Outcome Plots again help develop intuition

.absolute-bottom-right.tiny-font[
Fuel efficiency versus displacement, for 32 cars (1973–74 models). Source: Motor Trend, 1974
]

???

Figure from [Claus O. Wilke. Fundamentals of Data Visualization. O'Reilly, 2019.](https://clauswilke.com/dataviz)

---
class: middle center

## De-trending: Removing the underlying trend


---

## Time series often show long-term persistent trends

--

.small-font[
Housing prices follow long-term trend of exponential growth, overlaid with boom/bust cycles
]

.center[
```{r hpi-trends-CA, echo = FALSE, warning = FALSE, out.width = "65%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state == "California") %>%
  ggplot(aes(date_dec, hpi)) +
  geom_line(aes(y = hpi_trend_log), color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_continuous(name = NULL) +
  scale_y_log10(
    name = "House Price Index (Dec. 2000 = 100)",
    breaks = c(30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400),
    labels = c("", "", "50", "", "", "", "", "100", "", "300", "")
  ) +
  theme_minimal_hgrid() +
  theme(
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.absolute-bottom-right.tiny-font[
House Price Index (HPI) for California. Source: Freddie Mac
]

---

## Raw time series can be misleading

.center[
```{r hpi-no-trendline, echo = FALSE, warning = FALSE, out.width = "55%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date_dec, hpi)) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_continuous(name = NULL) +
  scale_y_log10(
    name = "House Price Index (Dec. 2000 = 100)",
    breaks = c(30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400),
    labels = c("", "", "50", "", "", "", "", "100", "", "300", "")
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.small-font[
Did housing prices in California decline substantially from 1990 to 1998?
]

.small-font[
Did housing prices in West Virginia recover by 2020?
]

.absolute-bottom-right.tiny-font[
US States House Price Index (HPI). Source: Freddie Mac
]

---

## Comparing the raw time series to the trendline helps

.center[
```{r hpi-trends, echo = FALSE, warning = FALSE, out.width = "55%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date_dec, hpi)) +
  geom_line(aes(y = hpi_trend_log), color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_continuous(name = NULL) +
  scale_y_log10(
    name = "House Price Index (Dec. 2000 = 100)",
    breaks = c(30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400),
    labels = c("", "", "50", "", "", "", "", "100", "", "300", "")
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.small-font[
Did housing prices in California decline substantially from 1990 to 1998?
]

.small-font[
Did housing prices in West Virginia recover by 2020?
]

.absolute-bottom-right.tiny-font[
US States House Price Index (HPI). Source: Freddie Mac
]

---

## Even better: Remove underlying trend

.center[
```{r hpi-detrended, echo = FALSE, warning = FALSE, out.width = "55%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date_dec, hpi/hpi_trend_log)) +
  geom_hline(yintercept = 1, color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_continuous(name = NULL) +
  scale_y_log10(
    name = "House Price Index (detrended)",
    limits = c(0.5, 2),
    breaks = c(0.5, 0.7, 1, 1.5, 2)
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.small-font[
Did housing prices in California decline substantially from 1990 to 1998? — yes
]

.small-font[
Did housing prices in West Virginia recover by 2020?  — no
]

.absolute-bottom-right.tiny-font[
US States House Price Index (HPI). Source: Freddie Mac
]

---

## Exponential versus linear de-trending

Two choices:

--

- Fit linear trendline, remove by subtraction

--

- Fit exponential trendline (linear in log space), remove by division

--

It is critical to make the correct choice for the dataset at hand

--

Any type of growth or decay process (change is proportional to present value) **must** be analyzed in log space

---

## Housing-price analysis in linear space looks wrong

.center[
```{r hpi-trends-linear, echo = FALSE, warning = FALSE, out.width = "75%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date_dec, hpi)) +
  geom_line(aes(y = hpi_trend_lin), color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_continuous(name = NULL) +
  scale_y_continuous(
    name = "House Price Index (Dec. 2000 = 100)",
    breaks = c(50, 100, 150, 200, 250, 300, 350),
    labels = c("", "100", "", "200", "", "300", "")
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.absolute-bottom-right.tiny-font[
US States House Price Index (HPI). Source: Freddie Mac
]

---

## Linear de-trending creates systematic deviations

.center[
```{r hpi-trends-linear-detrended, echo = FALSE, warning = FALSE, out.width = "75%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date_dec, hpi - hpi_trend_lin)) +
  geom_hline(yintercept = 1, color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_continuous(name = NULL) +
  scale_y_continuous(
    name = "House Price Index (detrended)",
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.absolute-bottom-right.tiny-font[
US States House Price Index (HPI). Source: Freddie Mac
]

---

## Log-space de-trending one more time for comparison

.center[
```{r hpi-detrended2, echo = FALSE, warning = FALSE, out.width = "75%", fig.width = 6, fig.asp = 0.618, dev = "svg"}

hpi_trends %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  ggplot(aes(date_dec, hpi/hpi_trend_log)) +
  geom_hline(yintercept = 1, color = "grey50", linewidth = 0.4) +
  geom_line(color = "#0072B2", linewidth = 0.75) +
  scale_x_continuous(name = NULL) +
  scale_y_log10(
    name = "House Price Index (detrended)",
    limits = c(0.5, 2),
    breaks = c(0.5, 0.7, 1, 1.5, 2)
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_minimal_hgrid() +
  theme(
    strip.text = element_text(size = 12),
    strip.background = element_rect(fill = "grey85"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )

```
]

.absolute-bottom-right.tiny-font[
US States House Price Index (HPI). Source: Freddie Mac
]

---
class: center middle

## Accounting for seasonal fluctuations

---

## Many time series show regular seasonal fluctuations

.center[
```{r keeling-curve, echo = FALSE, warning = FALSE, out.width = "70%", fig.width = 6, fig.asp = 0.618, dev = "svg"}
# convert to time series object
co2_ts <- ts(
  data = co2$co2_ave,
  start = 1959,       # data starts Jan 1959
  end = c(2023, 12),  # data ends Dec 2023
  frequency = 12      # we have 12 time points per year
)

# detrend via STL method
# Seasonal Decomposition of Time Series by Loess
# s.window is the span of the loess window; should be odd and at least 7
co2_stl <- stl(co2_ts, s.window = 7)

co2_detrended <- mutate(
  co2,
  seasonal = t(co2_stl$time.series)[1, ],
  trend = t(co2_stl$time.series)[2, ],
  remainder = t(co2_stl$time.series)[3, ]
)

ggplot(co2_detrended, aes(date_dec, co2_ave)) +
  geom_line(aes(color = "monthly"), linewidth = 0.6) +
  geom_line(aes(y = trend, color = "trend"), linewidth = 0.6) +
  scale_color_manual(
    name = NULL,
    values = c(monthly = "#0072B2", trend = "#D55E00"),
    breaks = c("monthly", "trend"),
    labels = c("monthly average  ", "long-term trend"),
    guide = guide_legend(
      override.aes = list(
        linewidth = 1
      )
    )
  ) +
  scale_y_continuous(
    limits = c(295, 430),
    breaks = c(300, 325, 350, 375, 400, 425),
    labels = c("300", "", "350", "", "400", ""),
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(1958, 2024),
    name = NULL,
    breaks = c(1960, 1970, 1980, 1990, 2000, 2010, 2020),
    labels = c("1960", "", "1980", "", "2000", "", "2020"),
    expand = c(0, 0)
  ) +
  theme_minimal_grid(14) +
  theme(
    legend.position = "top",
    legend.justification = "right",
    legend.box.spacing = unit(3.5, "pt"), # distance between legend and plot
    legend.background = element_rect(fill = "white", color = NA),
    legend.key.width = unit(14, "pt")
  )
```
]

.absolute-bottom-right.tiny-font[
CO<sub>2</sub> abundance in the atmosphere over time. Source: NOAA Global Monitoring Laboratory
]

---

## Many time series show regular seasonal fluctuations

.center[
```{r keeling-curve-zoomed, echo = FALSE, warning = FALSE, out.width = "70%", fig.width = 6, fig.asp = 0.618, dev = "svg"}
ggplot(co2_detrended, aes(date_dec, co2_ave)) +
  geom_line(aes(color = "monthly"), linewidth = 1) +
  geom_line(aes(y = trend, color = "trend"), linewidth = 1) +
  scale_color_manual(
    name = NULL,
    values = c(monthly = "#0072B2", trend = "#D55E00"),
    breaks = c("monthly", "trend"),
    labels = c("monthly average  ", "long-term trend")
  ) +
  scale_y_continuous(
    limits = c(383, 404),
    breaks = c(385, 390, 395, 400),
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(2009.5, 2014.5),
    name = NULL,
    breaks = 2010:2014,
    expand = c(0, 0)
  ) +
  theme_minimal_grid(14) +
  theme(
    legend.position = "top",
    legend.justification = "right",
    legend.box.spacing = unit(3.5, "pt"), # distance between legend and plot
    legend.background = element_rect(fill = "white", color = NA),
    legend.key.width = unit(14, "pt")
  )
```
]

---

## Seasonal Decomposition of Time Series by Loess (STL)

We can use STL to decompose a time series into:

1. long-term trend

2. seasonal effect

3. remainder (noise)


---

.center[
```{r keeling-curve-decomposition, echo = FALSE, warning = FALSE, out.width = "90%", fig.width = 9, fig.asp = 0.618, dev = "svg"}
facet_labels <- c("monthly average", "long-term trend", "seasonal fluctuations", "remainder")

co2_detrended |>
  rename(
    "monthly average" = co2_ave,
    "long-term trend" = trend,
    "seasonal fluctuations" = seasonal
  ) |>
  select(date_dec, `monthly average`, `seasonal fluctuations`, `long-term trend`, remainder) |>
  pivot_longer(-date_dec, names_to = "variable", values_to = "value") |>
  mutate(variable = factor(variable, levels = facet_labels)) |>
  filter(date_dec >= 1989) |>
  ggplot(aes(date_dec, value)) +
  geom_line(color = "#0072B2", linewidth = 0.6) +
  geom_point( # ghost points for scale range
    data = data.frame(
      variable = factor(
        rep(facet_labels, each = 2),
        levels = facet_labels
      ),
      x = 1990,
      y = c(324, 419, 324, 419, -5, 5, -1, 1)
    ),
    aes(x, y),
    color = NA,
    na.rm = TRUE
  ) +
  scale_y_continuous(
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(1989, 2023.2),
    name = NULL,
    breaks = c(1990, 1995, 2000, 2005, 2010, 2015, 2020),
    labels = c("1990", "", "2000", "", "2010", "", "2020"),
    expand = c(0, 0)
  ) +
  facet_wrap(facets = vars(variable), ncol = 1, scales = "free") +
  theme_minimal_grid() +
  theme(
    plot.margin = margin(3, 1.5, 3, 1.5),
    strip.text = element_text(size = 12)
  )
```
]

--

.small-font[
Magnitude of remainder should be small compared to magnitude of seasonal fluctuations
]


---

.center[
```{r keeling-curve-decomposition-zoomed, echo = FALSE, warning = FALSE, out.width = "90%", fig.width = 9, fig.asp = 0.618, dev = "svg"}
facet_labels <- c("monthly average", "long-term trend", "seasonal fluctuations", "remainder")

co2_detrended |>
  rename(
    "monthly average" = co2_ave,
    "long-term trend" = trend,
    "seasonal fluctuations" = seasonal
  ) |>
  select(date_dec, `monthly average`, `seasonal fluctuations`, `long-term trend`, remainder) |>
  pivot_longer(-date_dec, names_to = "variable", values_to = "value") |>
  mutate(variable = factor(variable, levels = facet_labels)) |>
  filter(date_dec > 2009.5 & date_dec < 2014.5) |>
  ggplot(aes(date_dec, value)) +
  geom_line(color = "#0072B2", linewidth = 0.6) +
  geom_point( # ghost points for scale range
    data = data.frame(
      variable = factor(
        rep(facet_labels, each = 2),
        levels = facet_labels
      ),
      x = 2012,
      y = c(383, 404, 383, 404, -5, 5, -1, 1)
    ),
    aes(x, y),
    color = NA,
    na.rm = TRUE
  ) +
  scale_y_continuous(
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(2009.5, 2014.5),
    name = NULL,
    breaks = 2010:2014,
    expand = c(0, 0)
  ) +
  facet_wrap(facets = vars(variable), ncol = 1, scales = "free") +
  theme_minimal_grid() +
  theme(
    plot.margin = margin(3, 1.5, 3, 1.5),
    strip.text = element_text(size = 12)
  )
```
]

.small-font[
Magnitude of remainder should be small compared to magnitude of seasonal fluctuations
]

---

## Other strategies for adjusting for seasonality

--

Simpler approaches:

- Fit model with fixed or random effects for specific seasons

--

More complex approaches:

- Perform Fourier or wavelet decomposition

---

## Take-home messages

--

1. Specify what your error bars show, and ask others to do the same

--

2. Consider frequency framing and Hypothetical Outcome Plots 

--

3. De-trend your data

--

4. Pay attention to linear vs. logarithmic processes

---

## Further reading

Materials by me:

.move-up-1em[
- Book: [Fundamentals of Data Visualization](https://clauswilke.com/dataviz/)
- UT Austin class: [Data Visualization in R](https://wilkelab.org/SDS375/)
- Fundamentals of Data Visualization: [Chapter 14: Visualizing trends](https://clauswilke.com/dataviz/visualizing-trends.html)
- Fundamentals of Data Visualization: [Chapter 16: Visualizing uncertainty](https://clauswilke.com/dataviz/visualizing-uncertainty.html)
]

Research literature on uncertainty visualization:

.move-up-1em[
- J. Hullman, P. Resnick, and E. Adar (2015). [Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444)
- S. Joslyn, S. Savelli (2021) [Visualizing Uncertainty for Non-Expert End Users: The Challenge of the Deterministic Construal Error](https://www.frontiersin.org/articles/10.3389/fcomp.2020.590232/full)
- L. Padilla, M. Kay, and J. Hullman (2022). [Uncertainty Visualization](
https://osf.io/preprints/psyarxiv/ebd6r)
]